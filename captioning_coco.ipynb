{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "captioning_coco.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMh1eD3N6IKGDq6DVFrtWYF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mobarakol/tutorial_captioning/blob/main/captioning_coco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pycocotools --user"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu27P-8cBpYs",
        "outputId": "ae58a906-4263-46ca-91fc-52ebf0d2533a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (2.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pycocotools) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.1.0->pycocotools) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O coco_captioning.zip https://www.dropbox.com/s/dngqe90t6owmsov/coco_captioning.zip?dl=0\n",
        "!unzip -q coco_captioning.zip -d ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWnAKQ6kDxTR",
        "outputId": "00b8f10d-99fc-4243-a28b-8738a60333af"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-23 14:16:03--  https://www.dropbox.com/s/dngqe90t6owmsov/coco_captioning.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6030:18::a27d:5012\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/dngqe90t6owmsov/coco_captioning.zip [following]\n",
            "--2022-08-23 14:16:03--  https://www.dropbox.com/s/raw/dngqe90t6owmsov/coco_captioning.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc45baac3e2dfbba90ace0073c9c.dl.dropboxusercontent.com/cd/0/inline/BrgSLEY56pliOA0ZRGfTgn1wSwur4Mzgxse7RXw80OQnflb74xlsjuqlFvq4lapOIlqPrU03lNfY5kJ09hLrCP-f8rY_x1gqL6pWqLbo8IsS9Xd9HdA3g4yBhaMqIr2s2j1Kg_7bEOuZXsq7MVSmQ_fB2ufcfGflKbMcD8EPfpTA5g/file# [following]\n",
            "--2022-08-23 14:16:04--  https://uc45baac3e2dfbba90ace0073c9c.dl.dropboxusercontent.com/cd/0/inline/BrgSLEY56pliOA0ZRGfTgn1wSwur4Mzgxse7RXw80OQnflb74xlsjuqlFvq4lapOIlqPrU03lNfY5kJ09hLrCP-f8rY_x1gqL6pWqLbo8IsS9Xd9HdA3g4yBhaMqIr2s2j1Kg_7bEOuZXsq7MVSmQ_fB2ufcfGflKbMcD8EPfpTA5g/file\n",
            "Resolving uc45baac3e2dfbba90ace0073c9c.dl.dropboxusercontent.com (uc45baac3e2dfbba90ace0073c9c.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6030:15::a27d:500f\n",
            "Connecting to uc45baac3e2dfbba90ace0073c9c.dl.dropboxusercontent.com (uc45baac3e2dfbba90ace0073c9c.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Bri-dq_T4lBMdK20Z9ml2oYt55qrv6-a59fLT_VfcciTHaBlw6yQ_qfF87r33odhYKQEm1Gsc5soOZsC2HSxQ9tw7TYhxkKwrNaRQ2iKxM6SBMQH8lVEab09cpWbauIu28NQfmi5ouARP5lXmNbezc2o4WpmWXJEJcgQInrvH7qf_lI34PQqZoklOFVEQSi_qEijzxmbjAkApnwv97mh433b76nFSGn6CMgav-0xhZdPTJ1jnYmRWmmq2ps8mb6eJjRy1-6yCpyxKBr8RMjdD7pMGt2wVO2uWhvI7eh4D7ATtcLvQ4iCIegAW9q-DZcgZD8czu6YmKRVFrEsc2w6LD4VOrhYxNuunfsKqUP8Hn3cFjMy2nfkajZnGZljHGyU0LcNQMyX2GfiiyRP3sfMkxwXQZP1DK1YW7QxU4rYfmn3HA/file [following]\n",
            "--2022-08-23 14:16:05--  https://uc45baac3e2dfbba90ace0073c9c.dl.dropboxusercontent.com/cd/0/inline2/Bri-dq_T4lBMdK20Z9ml2oYt55qrv6-a59fLT_VfcciTHaBlw6yQ_qfF87r33odhYKQEm1Gsc5soOZsC2HSxQ9tw7TYhxkKwrNaRQ2iKxM6SBMQH8lVEab09cpWbauIu28NQfmi5ouARP5lXmNbezc2o4WpmWXJEJcgQInrvH7qf_lI34PQqZoklOFVEQSi_qEijzxmbjAkApnwv97mh433b76nFSGn6CMgav-0xhZdPTJ1jnYmRWmmq2ps8mb6eJjRy1-6yCpyxKBr8RMjdD7pMGt2wVO2uWhvI7eh4D7ATtcLvQ4iCIegAW9q-DZcgZD8czu6YmKRVFrEsc2w6LD4VOrhYxNuunfsKqUP8Hn3cFjMy2nfkajZnGZljHGyU0LcNQMyX2GfiiyRP3sfMkxwXQZP1DK1YW7QxU4rYfmn3HA/file\n",
            "Reusing existing connection to uc45baac3e2dfbba90ace0073c9c.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1015173820 (968M) [application/zip]\n",
            "Saving to: ‘coco_captioning.zip’\n",
            "\n",
            "coco_captioning.zip 100%[===================>] 968.14M  7.88MB/s    in 1m 55s  \n",
            "\n",
            "2022-08-23 14:18:01 (8.43 MB/s) - ‘coco_captioning.zip’ saved [1015173820/1015173820]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This file defines different layers used for RNN and for image captioning.\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    A numerically stable version of the logistic sigmoid function.\n",
        "    There is no shape requirement for input x.\n",
        "    \"\"\"\n",
        "    pos_mask = (x >= 0)\n",
        "    neg_mask = (x < 0)\n",
        "    z = np.zeros_like(x)\n",
        "    z[pos_mask] = np.exp(-x[pos_mask])\n",
        "    z[neg_mask] = np.exp(x[neg_mask])\n",
        "    top = np.ones_like(x)\n",
        "    top[neg_mask] = z[neg_mask]\n",
        "    return top / (1 + z)\n",
        "\n",
        "\n",
        "def rnn_step_forward(x, prev_h, Wx, Wh, b):\n",
        "    \"\"\"\n",
        "    Run the forward pass for a single time stamp in Vanilla RNN with a tanh activation function:\n",
        "    next_h = tanh(Wx * x + Wh * prev_h + b).\n",
        "    Arguments:\n",
        "        x: input data for current time stamp with shape (N, D)\n",
        "        prev_h: hidden state from previous time stamp with shape (N, H)\n",
        "        Wx: weight matrix for input data with shape (D, H)\n",
        "        Wh: weight matrix for hidden states with shape (H, H)\n",
        "        b: bias with shape (H,)\n",
        "    Outputs:\n",
        "        next_h: hidden state after the forward step with shape (N, H)\n",
        "        cache: cache used for back-prop\n",
        "    \"\"\"\n",
        "    next_h = np.tanh(np.dot(x, Wx) + np.dot(prev_h, Wh) + b)\n",
        "    cache = x, prev_h, Wx, Wh, b, next_h\n",
        "    return next_h, cache\n",
        "\n",
        "\n",
        "def rnn_step_backward(dnext_h, cache):\n",
        "    \"\"\"\n",
        "    Run the backward pass for a single time stamp in Vanilla RNN with a tanh activation function:\n",
        "    dx = (1 - next_h^2) * Wx * dnext_h\n",
        "    dprev_h = (1 - next_h^2) * Wh * dnext_h\n",
        "    dWx = (1 - next_h^2) * x.T * dnext_h\n",
        "    dWh = (1 - next_h^2) * h.T * dnext_h\n",
        "    db = (1 - next_h^2) * dnext_h\n",
        "    Arguments:\n",
        "        dnext_h: gradient of hidden state with shape (N, H)\n",
        "        cache: cache used for back-prop\n",
        "    Outputs:\n",
        "        dx: gradient of input data with shape (N, D)\n",
        "        dprev_h: gradient of hidden state for previous time stamp with shape (N, H)\n",
        "        dWx: gradient of weight matrix for input data with shape (D, H)\n",
        "        dWh: gradient of weight matrix for hidden states with shape (H, H)\n",
        "        db: gradient of bias with shape (H,)\n",
        "    \"\"\"\n",
        "    x, prev_h, Wx, Wh, b, h = cache\n",
        "    dtanh = (1 - h ** 2) * dnext_h\n",
        "    dx = np.dot(dtanh, Wx.T)\n",
        "    dprev_h = np.dot(dtanh, Wh.T)\n",
        "    dWx = np.dot(x.T, dtanh)\n",
        "    dWh = np.dot(h.T, dtanh)\n",
        "    db = np.sum(dtanh, axis=0)\n",
        "    return dx, dprev_h, dWx, dWh, db\n",
        "\n",
        "\n",
        "def rnn_forward(x, h0, Wx, Wh, b):\n",
        "    \"\"\"\n",
        "    Run a forward pass for vanilla RNN on an entire sequence of data.\n",
        "    The input has N sequences, each of which is composed of T vectors, each of dimension D.\n",
        "    The hidden state size for the RNN is H.\n",
        "    Arguments:\n",
        "        x: input data for with shape (N, T, D)\n",
        "        h0: initial hidden state with shape (N, H)\n",
        "        Wx: weight matrix for input data with shape (D, H)\n",
        "        Wh: weight matrix for hidden states with shape (H, H)\n",
        "        b: bias with shape (H,)\n",
        "    Outputs:\n",
        "        h: hidden states after the forward step with shape (N, T, H)\n",
        "        cache: cache used for back-prop\n",
        "    \"\"\"\n",
        "    N, T, D = x.shape\n",
        "    _, H = h0.shape\n",
        "    x = np.swapaxes(x, 0, 1)  # swap axes for easier loops\n",
        "    h = np.zeros((T, N, H))  # initialize h\n",
        "    prev_h = h0\n",
        "    cache = []\n",
        "    for t in range(T):\n",
        "        next_h, cache_ = rnn_step_forward(x[t], prev_h, Wx, Wh, b)\n",
        "        prev_h = next_h\n",
        "        cache.append(cache_)\n",
        "        h[t] = prev_h\n",
        "    h = np.swapaxes(h, 0, 1)  # swap axes for correct format\n",
        "    return h, cache\n",
        "\n",
        "\n",
        "def rnn_backward(dh, cache):\n",
        "    \"\"\"\n",
        "    Run a backward pass for vanilla RNN from the gradient of all hidden states dh.\n",
        "    Arguments:\n",
        "        dh: gradient of all hidden states with shape (N, T, H)\n",
        "        cache: cache used for back-prop\n",
        "    Outputs:\n",
        "        dx: gradient of input data with shape (N, T, D)\n",
        "        dh0: gradient of initial hidden state with shape (N, H)\n",
        "        dWx: gradient of weight matrix for input data with shape (D, H)\n",
        "        dWh: gradient of weight matrix for hidden states with shape (H, H)\n",
        "        db: gradient of bias with shape (H,)\n",
        "    \"\"\"\n",
        "    dh = dh.copy()  # this is very important!\n",
        "    N, T, H = dh.shape\n",
        "    D = cache[0][0].shape[-1]  # extract parameter D fro initialization\n",
        "    dh = np.swapaxes(dh, 0, 1)  # swap axes for easier loops\n",
        "    # initializations for derivatives\n",
        "    dx, dh0, dWx, dWh, db = np.zeros((T, N, D)), np.zeros((N, H)), np.zeros((D, H)), np.zeros((H, H)), np.zeros((H,))\n",
        "    for t in range(T):\n",
        "        dx[t], dprev_h, dWx_, dWh_, db_ = rnn_step_backward(dh[t], cache[t])\n",
        "        # update parameters\n",
        "        dh[t] += dprev_h\n",
        "        dWx += dWx_\n",
        "        dWh += dWh_\n",
        "        db += db_\n",
        "    dh0 = dprev_h\n",
        "    dx = np.swapaxes(dx, 0, 1)  # swap axes for correct format\n",
        "    return dx, dh0, dWx, dWh, db\n",
        "\n",
        "\n",
        "def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b):\n",
        "    \"\"\"\n",
        "    Run the forward pass for a single time stamp in LSTM:\n",
        "    a = Wx * x + Wh * h + b;\n",
        "    a = [a_i, a_f, a_o, a_g];\n",
        "    i, f, o, g = sigmoid(a_i), sigmoid(a_f), sigmoid(a_o), tanh(a_g);\n",
        "    next_c = f ⊙ prev_c + i ⊙ g;\n",
        "    next_h = o ⊙ tanh(next_c).\n",
        "    The shapes are consistent with Vallina RNN.\n",
        "    Arguments:\n",
        "        x: input data for current time stamp with shape (N, D)\n",
        "        prev_h: hidden state from previous time stamp with shape (N, H)\n",
        "        prev_c: cell state from previous time stamp with shape (N, H)\n",
        "        Wx: weight matrix for input data with shape (D, 4H)\n",
        "        Wh: weight matrix for hidden states with shape (H, 4H)\n",
        "        b: bias with shape (4H,)\n",
        "    Outputs:\n",
        "        next_h: hidden state after the forward step with shape (N, H)\n",
        "        next_c: cell state after the forward step with shape (N, H)\n",
        "        cache: cache used for back-prop\n",
        "\n",
        "    \"\"\"\n",
        "    a = np.dot(x, Wx) + np.dot(prev_h, Wh) + b\n",
        "    H = a.shape[1] // 4\n",
        "    a_i, a_f, a_o, a_g = a[:, :H], a[:, H:2*H], a[:, 2*H:3*H], a[:, 3*H:]\n",
        "    i, f, o, g = sigmoid(a_i), sigmoid(a_f), sigmoid(a_o), np.tanh(a_g)\n",
        "    next_c = f * prev_c + i * g\n",
        "    next_h = o * np.tanh(next_c)\n",
        "    cache = x, prev_h, prev_c, Wx, Wh, b, i, f, o, g, next_h, next_c\n",
        "    return next_h, next_c, cache\n",
        "\n",
        "\n",
        "def lstm_step_backward(dnext_h, dnext_c, cache):\n",
        "    \"\"\"\n",
        "    Run the backward pass for a single time stamp in LSTM:\n",
        "    do = dnext_h ⊙ tanh(next_c);\n",
        "    ddnext_c = dnext_h ⊙ o;\n",
        "    df = dnext_c ⊙ prev_c;\n",
        "    dprev_c = dnext_c ⊙ f;\n",
        "    di = dnext_c ⊙ g;\n",
        "    dg = dnext_c ⊙ i;\n",
        "    da_i = di ⊙ i ⊙ (1 - i);\n",
        "    da_f = df ⊙ f ⊙ (1 - f);\n",
        "    da_o = do ⊙ o ⊙ (1 - o);\n",
        "    da_g = dg ⊙ (1 - g^2);\n",
        "    da = [da_i, da_f, da_o, da_g];\n",
        "    dx = da * Wx;\n",
        "    dh = da * Wh;\n",
        "    dWx = da * x.T;\n",
        "    dWh = da * h.T;\n",
        "    db = da.\n",
        "    Arguments:\n",
        "        dnext_h: gradient of hidden state with shape (N, H)\n",
        "        dnext_c: gradient of cell state with shape (N, H)\n",
        "        cache: cache used for back-prop\n",
        "    Outputs:\n",
        "        dx: gradient of input data with shape (N, D)\n",
        "        dprev_h: gradient of hidden state with shape (N, H)\n",
        "        dprev_c: gradient of cell state with shape (N, H)\n",
        "        dWx: gradient of weight matrix for input data with shape (D, 4H)\n",
        "        dWh: gradient of weight matrix for hidden states with shape (H, 4H)\n",
        "        db: gradient of bias with shape (4H,)\n",
        "    \"\"\"\n",
        "    x, prev_h, prev_c, Wx, Wh, b, i, f, o, g, h, c = cache\n",
        "    do = dnext_h * np.tanh(c)\n",
        "    dnext_c += dnext_h * o * (1 - np.tanh(c) ** 2)\n",
        "    dprev_c = dnext_c * f\n",
        "    dg = dnext_c * i\n",
        "    di = dnext_c * g\n",
        "    df = dnext_c * prev_c\n",
        "    da_i = di * i * (1 - i)\n",
        "    da_f = df * f * (1 - f)\n",
        "    da_o = do * o * (1 - o)\n",
        "    da_g = dg * (1 - g ** 2)\n",
        "    da = np.concatenate((da_i, da_f, da_o, da_g), axis=-1)\n",
        "    dx = np.dot(da, Wx.T)\n",
        "    dWx = np.dot(x.T, da)\n",
        "    dprev_h = np.dot(da, Wh.T)\n",
        "    dWh = np.dot(prev_h.T, da)\n",
        "    db = np.sum(da, axis=0)\n",
        "    return dx, dprev_h, dprev_c, dWx, dWh, db\n",
        "\n",
        "\n",
        "def lstm_forward(x, h0, Wx, Wh, b):\n",
        "    \"\"\"\n",
        "    Run a forward pass for LSTM on an entire sequence of data.\n",
        "    The dimensions are consistent with Vallina RNN.\n",
        "    Arguments:\n",
        "        x: input data for with shape (N, T, D)\n",
        "        h0: initial hidden state with shape (N, H)\n",
        "        Wx: weight matrix for input data with shape (D, H)\n",
        "        Wh: weight matrix for hidden states with shape (H, H)\n",
        "        b: bias with shape (H,)\n",
        "    Outputs:\n",
        "        h: hidden states after the forward step with shape (N, T, H)\n",
        "        cache: cache used for back-prop\n",
        "    \"\"\"\n",
        "    N, T, D = x.shape\n",
        "    _, H = h0.shape\n",
        "    x = np.swapaxes(x, 0, 1)  # swap axes for easier loops\n",
        "    h = np.zeros((T, N, H))\n",
        "    prev_c = np.zeros((N, H))\n",
        "    prev_h = h0\n",
        "    cache = []\n",
        "    for i in range(T):\n",
        "        prev_h, prev_c, cache_ = lstm_step_forward(x[i], prev_h, prev_c, Wx, Wh, b)\n",
        "        h[i] = prev_h\n",
        "        cache.append(cache_)\n",
        "    h = np.swapaxes(h, 0, 1)  # swap back for correct format\n",
        "    return h, cache\n",
        "\n",
        "\n",
        "def lstm_backward(dh, cache):\n",
        "    \"\"\"\n",
        "    Run a backward pass for LSTM from the derivative of all hidden states dh.\n",
        "    Arguments:\n",
        "        dh: gradient of all hidden states with shape (N, T, H)\n",
        "        cache: cache used for back-prop\n",
        "    Outputs:\n",
        "        dx: gradient of input data with shape (N, T, D)\n",
        "        dh0: gradient of initial hidden state with shape (N, H)\n",
        "        dWx: gradient of weight matrix for input data with shape (D, H)\n",
        "        dWh: gradient of weight matrix for hidden states with shape (H, H)\n",
        "        db: gradient of bias with shape (H,)\n",
        "    \"\"\"\n",
        "    dh = dh.copy()  # very important!\n",
        "    N, T, H = dh.shape\n",
        "    D = cache[0][0].shape[-1]  # extract parameter D\n",
        "    dh = np.swapaxes(dh, 0, 1)  # swap axes for easier loops\n",
        "    # initialization of derivatives\n",
        "    dx, dWx, dWh, db, dprev_h = np.zeros((T, N, D)), np.zeros((D, 4*H)), np.zeros((H, 4*H)), np.zeros((4*H,)), np.zeros((N, H))\n",
        "    dprev_c = np.zeros(dprev_h.shape)\n",
        "    for t in reversed(range(T)):\n",
        "        dh[t] += dprev_h\n",
        "        dx[t], dprev_h, dprev_c, dWx_, dWh_, db_ = lstm_step_backward(dh[t], dprev_c, cache[t])\n",
        "        dWx += dWx_\n",
        "        dWh += dWh_\n",
        "        db += db_\n",
        "    dh0 = dprev_h\n",
        "    dx = np.swapaxes(dx, 0, 1)\n",
        "    return dx, dh0, dWx, dWh, db\n",
        "\n",
        "\n",
        "def temporal_affine_forward(x, W, b):\n",
        "    \"\"\"\n",
        "    Run a forward pass for temporal affine layer. The dimensions are consistent with RNN/LSTM forward passes.\n",
        "    Arguments:\n",
        "        x: input data with shape (N, T, D)\n",
        "        W: weight matrix for input data with shape (D, M)\n",
        "        b: bias with shape (M,)\n",
        "    Outputs:\n",
        "        out: output data with shape (N, T, M)\n",
        "        cache: cache for back-prop\n",
        "    \"\"\"\n",
        "    N, T, D = x.shape\n",
        "    M = b.shape[0]\n",
        "    out = np.dot(x.reshape(N * T, D), W).reshape(N, T, M) + b\n",
        "    cache = x, W, b, out\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def temporal_affine_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Run a backward pass for temporal affine layer. The dimensions are consistent with RNN/LSTM forward passes.\n",
        "    Arguments:\n",
        "        dout: gradient of output data with shape (N, T, M)\n",
        "        cache: cache for back-prop\n",
        "    Outputs:\n",
        "        dx: gradient of input data with shape (N, T, D)\n",
        "        dW: gradient of weight matrix with shape (D, M)\n",
        "        db: gradient of bias with shape (M,)\n",
        "    \"\"\"\n",
        "    x, W, b, out = cache\n",
        "    N, T, D = x.shape\n",
        "    M = b.shape[0]\n",
        "    dx = np.dot(dout.reshape(N * T, M), W.T).reshape(N, T, D)\n",
        "    dw = np.dot(dout.reshape(N * T, M).T, x.reshape(N * T, D)).T\n",
        "    db = dout.sum(axis=(0, 1))\n",
        "    return dx, dw, db\n",
        "\n",
        "\n",
        "def temporal_softmax_loss(x, y, mask):\n",
        "    \"\"\"\n",
        "    This function is adapted from CS231n.\n",
        "    A temporal version of softmax loss for use in RNNs.\n",
        "    The vocabulary has size V for each time step of a time series of length T, with a batch size of N.\n",
        "    Cross-entropy loss is calculated, summed and averaged over all time steps across the batch.\n",
        "    Arguments:\n",
        "    - x: input scores for all vocabulary elements with shape of (N, T, V)\n",
        "    - y: ground-truth indices at each time step with shape of (N, T), each element of which is in [0, V)\n",
        "    - mask: boolean array with shape of (N, T) indicating whether the scores at x[n, t] should contribute to the loss\n",
        "    Outputs:\n",
        "    - loss: float of loss\n",
        "    - dx: gradient of loss with respect to scores x\n",
        "    \"\"\"\n",
        "    N, T, V = x.shape\n",
        "    x_flat = x.reshape(N * T, V)\n",
        "    y_flat = y.reshape(N * T)\n",
        "    mask_flat = mask.reshape(N * T)\n",
        "    probs = np.exp(x_flat - np.max(x_flat, axis=1, keepdims=True))\n",
        "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
        "    loss = -np.sum(mask_flat * np.log(probs[np.arange(N * T), y_flat])) / N\n",
        "    dx_flat = probs.copy()\n",
        "    dx_flat[np.arange(N * T), y_flat] -= 1\n",
        "    dx_flat /= N\n",
        "    dx_flat *= mask_flat[:, None]\n",
        "    dx = dx_flat.reshape(N, T, V)\n",
        "    return loss, dx\n",
        "\n",
        "\n",
        "def word_embedding_forward(x, W):\n",
        "    \"\"\"\n",
        "    Run a forward pass for word embeddings.\n",
        "    The dimensions are consistent with parameters in temporal softmax loss.\n",
        "    Arguments:\n",
        "    - x: integer array with shape of (N, T) giving indices of words, each of which lies in [0, V)\n",
        "    - W: weight matrix with shape of (V, D) giving word vectors for all words.\n",
        "    Outputs:\n",
        "    - out: array with shape of (N, T, D) giving word vectors for all input words.\n",
        "    - cache: cache for back-prop\n",
        "    \"\"\"\n",
        "    out = W[x, :]\n",
        "    cache = x, W\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def word_embedding_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Run a backward pass for word embeddings.\n",
        "    The dimensions are consistent with parameters in temporal softmax loss.\n",
        "    Arguments:\n",
        "    - dout: gradient of output with shape of (N, T, D)\n",
        "    - cache: cache used for back-prop\n",
        "    Outputs:\n",
        "    - dW: gradient of weight matrix with shape of (V, D)\n",
        "    \"\"\"\n",
        "    x, W = cache\n",
        "    dW = np.zeros(W.shape)\n",
        "    np.add.at(dW, x, dout)\n",
        "    return dW\n",
        "\n",
        "import numpy as np\n",
        "from builtins import object\n",
        "#from layers import *\n",
        "\n",
        "\n",
        "class RNNImageCaption(object):\n",
        "    \"\"\"\n",
        "    Define a RNN_image_captioning class, the instance of which outputs captions given image features.\n",
        "    \"\"\"\n",
        "    def __init__(self, word_to_idx, input_dim=512, wordvec_dim=128, hidden_dim=128, cell_type='rnn', dtype=np.float32):\n",
        "        \"\"\"\n",
        "        Initialization of instance in RNN_image_captioning.\n",
        "        Arguments:\n",
        "             word_to_idx: dictionary of word-index vocabulary table with V entries\n",
        "             input_dim: input image feature dimension D\n",
        "             wordvec_dim: word vector dimension W\n",
        "             hidden_dim: hidden state dimension H in RNN\n",
        "             cell_type: either 'rnn' or 'lstm' setting the RNN type\n",
        "             dtype: numpy datatype - float32 for training and float64 for numerical gradient check\n",
        "        \"\"\"\n",
        "        if cell_type not in ['rnn', 'lstm']:\n",
        "            raise ValueError('Unknown cell type of \"%s\"' % cell_type)\n",
        "        self.cell_type = cell_type\n",
        "        self.input_dim = input_dim\n",
        "        self.wordvec_dim = wordvec_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dtype = dtype\n",
        "        self.params = {}\n",
        "        # save indices of NULL, START and END\n",
        "        self.null = word_to_idx['<NULL>']\n",
        "        self.start = word_to_idx.get('<START>')\n",
        "        self.end = word_to_idx.get('<END>')\n",
        "        # initialization of word vectors\n",
        "        self.params['W_embed'] = np.random.randn(len(word_to_idx), wordvec_dim) / 100\n",
        "        # initialization of hidden state projection parameters for CNN\n",
        "        self.params['W_proj'] = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)\n",
        "        self.params['b_proj'] = np.zeros(hidden_dim)\n",
        "        # initialization of RNN parameters\n",
        "        dimension_factor = {'rnn':1, 'lstm':4}[cell_type]\n",
        "        self.params['Wx'] = np.random.randn(wordvec_dim, dimension_factor * hidden_dim) / np.sqrt(wordvec_dim)\n",
        "        self.params['Wh'] = np.random.randn(hidden_dim, dimension_factor * hidden_dim) / np.sqrt(hidden_dim)\n",
        "        self.params['b'] = np.zeros(dimension_factor * hidden_dim)\n",
        "        # initialization of vocab weights\n",
        "        self.params['W_vocab'] = np.random.randn(hidden_dim, len(word_to_idx)) / np.sqrt(hidden_dim)\n",
        "        self.params['b_vocab'] = np.zeros(len(word_to_idx))\n",
        "        # cast dtype\n",
        "        for para_name, param in self.params.items():\n",
        "            self.params[para_name] = param.astype(self.dtype)\n",
        "\n",
        "    def loss(self, features, captions):\n",
        "        \"\"\"\n",
        "        Calculate the training loss for captioning RNN.\n",
        "        Arguments:\n",
        "             features: input image features with shape of (N, D)\n",
        "             captions: an integer array of ground-truth captions with shape of (N, T) with elements in [0, V)\n",
        "        Outputs:\n",
        "            loss: float of loss value\n",
        "            grads: dictionary of gradients of parameters in self.params\n",
        "        \"\"\"\n",
        "        # Cut out the last words of captions as input, and the expected output is everything but the first words.\n",
        "        # Note that the first element of captions would be the START token.\n",
        "        captions_in = captions[:, :-1]  # entire caption except for the last words\n",
        "        captions_out = captions[:, 1:]  # entire caption except for the first words\n",
        "\n",
        "        mask = (captions_out != self.null)  # Indicating non-NULL indices to be used\n",
        "\n",
        "        # unpack initialized parameters\n",
        "        W_embed = self.params['W_embed']\n",
        "        W_proj, b_proj = self.params['W_proj'], self.params['b_proj']\n",
        "        Wx, Wh, b = self.params['Wx'], self.params['Wh'], self.params['b']\n",
        "        W_vocab, b_vocab = self.params['W_vocab'], self.params['b_vocab']\n",
        "\n",
        "        # loss calculation\n",
        "        h0 = np.dot(features, W_proj) + b_proj  # initial hidden state from image features - (N, H)\n",
        "        x, cache_embed = word_embedding_forward(captions_in, W_embed)  # transform words in captions_in - (N, T, W)\n",
        "        if self.cell_type == 'rnn':  # use Vanilla RNN to produce hidden states from input word vectors - (N, T, H)\n",
        "            h, cache_rnn = rnn_forward(x, h0, Wx, Wh, b)\n",
        "        else:                        # use LSTM to produce hidden states from input word vectors - (N, T, H)\n",
        "            h, cache_lstm = lstm_forward(x, h0, Wx, Wh, b)\n",
        "        scores, cache_temporal = temporal_affine_forward(h, W_vocab, b_vocab)  # compute scores - (N, T, V)\n",
        "        loss, dscores = temporal_softmax_loss(scores, captions_out, mask)  # compute loss, ignoring <NULL> tokens\n",
        "\n",
        "        # gradients calculation using back-props\n",
        "        dh, dW_vocab, db_vocab = temporal_affine_backward(dscores, cache_temporal)\n",
        "        if self.cell_type == 'rnn':\n",
        "            dx, dh0, dWx, dWh, db = rnn_backward(dh, cache_rnn)\n",
        "        else:\n",
        "            dx, dh0, dWx, dWh, db = lstm_backward(dh, cache_lstm)\n",
        "        dW_embed = word_embedding_backward(dx, cache_embed)\n",
        "        dW_proj = np.dot(features.T, dh0)\n",
        "        db_proj = np.sum(dh0, axis=0)\n",
        "\n",
        "        # put gradients into dictionary\n",
        "        # note that the keys have the same strings as in parameters for convenience during extraction\n",
        "        grads = {}\n",
        "        grads['W_embed'] = dW_embed\n",
        "        grads['W_proj'], grads['b_proj'] = dW_proj, db_proj\n",
        "        grads['Wx'], grads['Wh'], grads['b'] = dWx, dWh, db\n",
        "        grads['W_vocab'], grads['b_vocab'] = dW_vocab, db_vocab\n",
        "\n",
        "        return loss, grads\n",
        "\n",
        "    def generate_captions(self, features, max_length=30):\n",
        "        \"\"\"\n",
        "        Generate captions from the image features.\n",
        "        Arguments:\n",
        "             features: input image features with shape of (N, D)\n",
        "             max_length: maximum length T of generated caption\n",
        "        Outputs:\n",
        "            captions: array of generated captions with shape of (N, T) and each element lies in [0, V)\n",
        "        \"\"\"\n",
        "        N, D = features.shape\n",
        "        captions = self.null * np.ones((N, max_length), dtype=np.int32)  # initialize captions to <NULL>s\n",
        "\n",
        "        # Unpack parameters\n",
        "        W_embed = self.params['W_embed']\n",
        "        W_proj, b_proj = self.params['W_proj'], self.params['b_proj']\n",
        "        Wx, Wh, b = self.params['Wx'], self.params['Wh'], self.params['b']\n",
        "        W_vocab, b_vocab = self.params['W_vocab'], self.params['b_vocab']\n",
        "\n",
        "        # Generate captions\n",
        "        h0 = np.dot(features, W_proj) + b_proj  # initial hidden state from image features - (N, H)\n",
        "        captions[:, 0] = self.start  # set <START> tokens to the generated captions\n",
        "        capt = self.start * np.ones((N, 1), dtype=np.int32)  # set <START> tokens to the generated word for each time step\n",
        "        prev_h = h0\n",
        "        prev_c = np.zeros(h0.shape)  # initialize the cell state to zeros\n",
        "        for t in range(max_length):\n",
        "            x, _ = word_embedding_forward(capt, W_embed)  # word embedding\n",
        "            # get next hidden state\n",
        "            if self.cell_type == 'rnn':\n",
        "                h, _ = rnn_step_forward(np.squeeze(x), prev_h, Wx, Wh, b)  # note: squeeze for dimension match\n",
        "                prev_h = h\n",
        "            else:\n",
        "                h, c, _ = lstm_step_forward(np.squeeze(x), prev_h, prev_c, Wx, Wh, b)\n",
        "                prev_h = h\n",
        "                prev_c = c\n",
        "            scores, _ = temporal_affine_forward(h[:, np.newaxis, :], W_vocab, b_vocab)  # note: new axis for dimension match\n",
        "            capt = np.squeeze(np.argmax(scores, axis=2))\n",
        "            captions[:, t] = capt  # store generated captions\n",
        "\n",
        "        return captions\n",
        "\n",
        "\n",
        "# This code is modified from CS231n.\n",
        "import json\n",
        "import numpy as np\n",
        "import h5py\n",
        "import urllib.request, urllib.error, urllib.parse, tempfile, os\n",
        "from imageio import imread\n",
        "\n",
        "DATA_DIR = 'data/coco_captioning'  # define the dataset path\n",
        "\n",
        "\n",
        "def load_coco_dataset(data_dir=DATA_DIR, PCA_features=True, max_train=None):\n",
        "    \"\"\"\n",
        "    Load Microsoft COCO dataset.\n",
        "    Arguments:\n",
        "        data_dir: path to the dataset\n",
        "        PCA_features: whether use PCA features\n",
        "        max_train: max number of training data if only a subset is needed\n",
        "    Outputs:\n",
        "        data: dictionary containing different datasets with their names\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "    caption_file = os.path.join(data_dir, 'coco2014_captions.h5')\n",
        "    with h5py.File(caption_file, 'r') as f:  # read caption file with h5py\n",
        "        for k, v in f.items():\n",
        "            data[k] = np.asarray(v)\n",
        "\n",
        "    # extract training features\n",
        "    if PCA_features:\n",
        "        train_feature_file = os.path.join(data_dir, 'train2014_vgg16_fc7_pca.h5')\n",
        "    else:\n",
        "        train_feature_file = os.path.join(data_dir, 'train2014_vgg16_fc7.h5')\n",
        "    with h5py.File(train_feature_file, 'r') as f:\n",
        "        data['train_features'] = np.asarray(f['features'])\n",
        "\n",
        "    # extract validation features\n",
        "    if PCA_features:\n",
        "        val_feature_file = os.path.join(data_dir, 'val2014_vgg16_fc7_pca.h5')\n",
        "    else:\n",
        "        val_feature_file = os.path.join(data_dir, 'val2014_vgg16_fc7.h5')\n",
        "    with h5py.File(val_feature_file, 'r') as f:\n",
        "        data['val_features'] = np.asarray(f['features'])\n",
        "\n",
        "    # extract index-to-word and word-to-index into dictionary\n",
        "    dict_file = os.path.join(data_dir, 'coco2014_vocab.json')\n",
        "    with open(dict_file, 'r') as f:\n",
        "        dict_data = json.load(f)\n",
        "        for k, v in dict_data.items():\n",
        "            data[k] = v\n",
        "\n",
        "    # read image files from website, note that some of them might not be available for now\n",
        "    train_url_file = os.path.join(data_dir, 'train2014_urls.txt')  # this file includes urls for the training images\n",
        "    with open(train_url_file, 'r') as f:\n",
        "        train_urls = np.asarray([line.strip() for line in f])\n",
        "    data['train_urls'] = train_urls\n",
        "\n",
        "    val_url_file = os.path.join(data_dir, 'val2014_urls.txt')  # this file includes urls for the validation images\n",
        "    with open(val_url_file, 'r') as f:\n",
        "        val_urls = np.asarray([line.strip() for line in f])\n",
        "    data['val_urls'] = val_urls\n",
        "\n",
        "    # Maybe subsample the training data\n",
        "    if max_train is not None:\n",
        "        num_train = data['train_captions'].shape[0]\n",
        "        mask = np.random.randint(num_train, size=max_train)\n",
        "        data['train_captions'] = data['train_captions'][mask]\n",
        "        data['train_image_idxs'] = data['train_image_idxs'][mask]\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def sample_coco_minibatch(data, batch_size=100, split='train'):\n",
        "    \"\"\"\n",
        "    Sample a small amount of data.\n",
        "    Arguments:\n",
        "        data: loaded dataset from COCO\n",
        "        batch_size: int for batch size\n",
        "        split: string of either 'train' or 'val' indicating training/validation set\n",
        "    Outputs:\n",
        "        captions: ground truth captions of the images\n",
        "        image_features: features of the images\n",
        "        urls: image urls for image display\n",
        "    \"\"\"\n",
        "    split_size = data['%s_captions' % split].shape[0]\n",
        "    mask = np.random.choice(split_size, batch_size)\n",
        "    captions = data['%s_captions' % split][mask]\n",
        "    image_idxs = data['%s_image_idxs' % split][mask]\n",
        "    image_features = data['%s_features' % split][image_idxs]\n",
        "    urls = data['%s_urls' % split][image_idxs]\n",
        "    return captions, image_features, urls\n",
        "\n",
        "\n",
        "def decode_captions(captions, idx_to_word):\n",
        "    \"\"\"\n",
        "    Decode output captions into worded captions.\n",
        "    Arguments:\n",
        "        captions: output captions to be decoded\n",
        "        idx_to_word: dictionary of word-index vocabulary table\n",
        "    Outputs:\n",
        "        decoded: decoded worded captions\n",
        "    \"\"\"\n",
        "    singleton = False\n",
        "    if captions.ndim == 1:\n",
        "        singleton = True\n",
        "        captions = captions[None]\n",
        "    decoded = []\n",
        "    N, T = captions.shape\n",
        "    for i in range(N):\n",
        "        words = []\n",
        "        for t in range(T):\n",
        "            word = idx_to_word[captions[i, t]]\n",
        "            if word != '<NULL>':\n",
        "                words.append(word)\n",
        "            if word == '<END>':\n",
        "                break\n",
        "        decoded.append(' '.join(words))\n",
        "    if singleton:\n",
        "        decoded = decoded[0]\n",
        "    return decoded\n",
        "\n",
        "\n",
        "def image_from_url(url):\n",
        "    \"\"\"\n",
        "    Read an image from a URL. Returns a numpy array with the pixel data.\n",
        "    Arguments:\n",
        "        url: urls for images for display\n",
        "    Outputs:\n",
        "        img: numpy array for the image\n",
        "    \"\"\"\n",
        "    try:\n",
        "        f = urllib.request.urlopen(url)\n",
        "        _, fname = tempfile.mkstemp()\n",
        "        with open(fname, 'wb') as ff:\n",
        "            ff.write(f.read())\n",
        "        img = imread(fname)\n",
        "        # os.remove(fname)\n",
        "        return img\n",
        "    except urllib.error.HTTPError as e:\n",
        "        print('HTTP Error: ', e.code, url)\n",
        "    except urllib.error.URLError as e:\n",
        "        print('URL Error: ', e.reason, url)\n",
        "\n",
        "\n",
        "# This file includes SGD and Adam for parameter update.\n",
        "import numpy as np\n",
        "def sgd(w, dw, params={}):\n",
        "    \"\"\"\n",
        "    Perform Vanilla SGD for parameter update.\n",
        "    Arguments:\n",
        "        w: numpy array of current weight\n",
        "        dw: numpy array of gradient of loss w.r.t. current weight\n",
        "        params: dictionary containing hyper-parameters\n",
        "            - lr: float of learning rate\n",
        "    Outputs:\n",
        "        next_w: updated weight\n",
        "        params: updated dictionary of hyper-parameters\n",
        "    \"\"\"\n",
        "    # set default parameters\n",
        "    params.setdefault('lr', 1e-2)\n",
        "    # update w\n",
        "    next_w = w - params['lr'] * dw\n",
        "\n",
        "    return next_w, params\n",
        "\n",
        "def adam(w, dw, params={}):\n",
        "    \"\"\"\n",
        "    Perform Adam update rule for parameter update.\n",
        "    This update rule incorporates moving averages of both the gradient and its square and a bias correction term.\n",
        "    Arguments:\n",
        "        w: numpy array of current weight\n",
        "        dw: numpy array of gradient of loss w.r.t. current weight\n",
        "        params: dictionary containing hyper-parameters\n",
        "            - lr: float of learning rate\n",
        "            - beta1: float of decay rate for moving average of first moment of gradient\n",
        "            - beta2: float of decay rate for moving average of second moment of gradient\n",
        "            - epsilon: float of a small value used for smoothing to avoid dividing by zero\n",
        "            - m: numpy array of moving average of gradient with the sameshape of w\n",
        "            - v: moving average of squared gradient with the sameshape of w\n",
        "            - t: int of iteration number\n",
        "    Outputs:\n",
        "        next_w: updated weight\n",
        "        params: updated dictionary of hyper-parameters\n",
        "    \"\"\"\n",
        "    # set default parameters\n",
        "    params.setdefault('lr', 1e-2)\n",
        "    params.setdefault('beta1', 0.9)\n",
        "    params.setdefault('beta2', 0.999)\n",
        "    params.setdefault('epsilon', 1e-8)\n",
        "    params.setdefault('m', np.zeros_like(w))\n",
        "    params.setdefault('v', np.zeros_like(w))\n",
        "    params.setdefault('t', 0)\n",
        "    # update w\n",
        "    lr, beta1, beta2, epsilon, m, v, t = \\\n",
        "        params['lr'], params['beta1'], params['beta2'], params['epsilon'], params['m'], params['v'], params['t']\n",
        "    m = beta1 * m + (1 - beta1) * dw\n",
        "    v = beta2 * v + (1 - beta2) * dw ** 2\n",
        "    t += 1\n",
        "    alpha = params['lr'] * np.sqrt(1 - beta2 ** t) / (1 - beta1 ** t)\n",
        "    w -= alpha * (m / (np.sqrt(v) + epsilon))\n",
        "    params['t'] = t\n",
        "    params['m'] = m\n",
        "    params['v'] = v\n",
        "    next_w = w\n",
        "\n",
        "    return next_w, params\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from builtins import object\n",
        "#import update_method\n",
        "\n",
        "\n",
        "class CaptionTrain(object):\n",
        "    \"\"\"\n",
        "    This class defines the training of the caption generator using SGD.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, model, **kwargs):\n",
        "        \"\"\"\n",
        "        Initialization of CaptionTrain instance.\n",
        "        Arguments:\n",
        "            data: dictionary of training and validation dataset\n",
        "            model: model object from RNNImageCaption\n",
        "            optional arguments:\n",
        "                update: string of update method of either 'sgd' or 'adam'\n",
        "                update_params: dictionary of hyper-parameters for update method\n",
        "                lr_decay: float of learning rate decay\n",
        "                batch_size: integer of batch size for loss and gradient computation\n",
        "                num_epochs: integer of number of epochs\n",
        "                print_freq: integer of loss printing frequency steps\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.model = model\n",
        "\n",
        "        self.update = kwargs.pop('update', 'sgd')\n",
        "        self.update_params = kwargs.pop('update_params', {})\n",
        "        self.lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "        self.batch_size = kwargs.pop('batch_size', 100)\n",
        "        self.num_epochs = kwargs.pop('num_epochs', 10)\n",
        "        self.print_freq = kwargs.pop('print_freq', 10)\n",
        "        # throw error if more parameters are detected\n",
        "        if len(kwargs) > 0:\n",
        "            unreg_args = ', '.join('\"%s\"' % k for k in list(kwargs.keys()))\n",
        "            raise ValueError('Unrecognized arguments %s' % unreg_args)\n",
        "        # throw error if the update method is not supported\n",
        "        if self.update not in ['sgd', 'adam']:\n",
        "            raise ValueError('Unsupported update method %s' % self.update)\n",
        "        #self.update_method = getattr(update_method, self.update)  # get update method from file \"update_method.py\"\n",
        "        if self.update == 'sgd':\n",
        "            self.update_method = sgd\n",
        "        else:\n",
        "            self.update_method = adam\n",
        "\n",
        "\n",
        "        # initialize training parameters\n",
        "        self.epoch = 0\n",
        "        self.best_params = {}\n",
        "        self.best_val_acc = 0.0\n",
        "        self.loss_history = []\n",
        "        self.train_acc_history = []\n",
        "        self.val_acc_history = []\n",
        "\n",
        "        # perform a deep copy of the update method parameters for each model parameter\n",
        "        self.update_params_all = {}\n",
        "        for param in self.model.params:\n",
        "            self.update_params_all[param] = {k:v for k, v in self.update_params.items()}\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "        \"\"\"\n",
        "        num_train = self.data['train_features'].shape[0]\n",
        "        num_iter_epoch = max(num_train // self.batch_size, 1)\n",
        "        num_iters = num_iter_epoch * self.num_epochs\n",
        "\n",
        "        for t in range(num_iters):\n",
        "            self._gradient_update()\n",
        "            if t % self.print_freq == 0:\n",
        "                print('(Iteration %d / %d) loss: %f' % (t + 1, num_iters, self.loss_history[-1]))\n",
        "            if (t + 1) % num_iter_epoch == 0:\n",
        "                self.epoch += 1\n",
        "                for param in self.update_params_all:\n",
        "                    self.update_params_all[param]['lr'] *= self.lr_decay\n",
        "\n",
        "    def _gradient_update(self):\n",
        "        \"\"\"\n",
        "        Conduct a gradient update for training.\n",
        "        \"\"\"\n",
        "        # sample minibatch\n",
        "        captions, image_features, urls = sample_coco_minibatch(self.data, self.batch_size, split='train')\n",
        "        # compute loss and gradient\n",
        "        loss, gradients = self.model.loss(image_features, captions)\n",
        "        self.loss_history.append(loss)\n",
        "        # parameter update\n",
        "        for para_name, param in self.model.params.items():\n",
        "            dparam = gradients[para_name]\n",
        "            next_param, params = self.update_method(param, dparam, self.update_params_all[para_name])\n",
        "            self.model.params[para_name] = next_param\n",
        "            self.update_params_all[para_name] = params"
      ],
      "metadata": {
        "id": "8YdjniIbHHtD"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import h5py\n",
        "import urllib.request, urllib.error, urllib.parse, tempfile, os\n",
        "from imageio import imread\n",
        "\n",
        "DATA_DIR = 'coco_captioning'  # define the dataset path\n",
        "\n",
        "\n",
        "def load_coco_dataset(data_dir=DATA_DIR, PCA_features=True, max_train=None):\n",
        "    \"\"\"\n",
        "    Load Microsoft COCO dataset.\n",
        "    Arguments:\n",
        "        data_dir: path to the dataset\n",
        "        PCA_features: whether use PCA features\n",
        "        max_train: max number of training data if only a subset is needed\n",
        "    Outputs:\n",
        "        data: dictionary containing different datasets with their names\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "    caption_file = os.path.join(data_dir, 'coco2014_captions.h5')\n",
        "    with h5py.File(caption_file, 'r') as f:  # read caption file with h5py\n",
        "        for k, v in f.items():\n",
        "            data[k] = np.asarray(v)\n",
        "\n",
        "    # extract training features\n",
        "    if PCA_features:\n",
        "        train_feature_file = os.path.join(data_dir, 'train2014_vgg16_fc7_pca.h5')\n",
        "    else:\n",
        "        train_feature_file = os.path.join(data_dir, 'train2014_vgg16_fc7.h5')\n",
        "    with h5py.File(train_feature_file, 'r') as f:\n",
        "        data['train_features'] = np.asarray(f['features'])\n",
        "\n",
        "    # extract validation features\n",
        "    if PCA_features:\n",
        "        val_feature_file = os.path.join(data_dir, 'val2014_vgg16_fc7_pca.h5')\n",
        "    else:\n",
        "        val_feature_file = os.path.join(data_dir, 'val2014_vgg16_fc7.h5')\n",
        "    with h5py.File(val_feature_file, 'r') as f:\n",
        "        data['val_features'] = np.asarray(f['features'])\n",
        "\n",
        "    # extract index-to-word and word-to-index into dictionary\n",
        "    dict_file = os.path.join(data_dir, 'coco2014_vocab.json')\n",
        "    with open(dict_file, 'r') as f:\n",
        "        dict_data = json.load(f)\n",
        "        for k, v in dict_data.items():\n",
        "            data[k] = v\n",
        "\n",
        "    # read image files from website, note that some of them might not be available for now\n",
        "    train_url_file = os.path.join(data_dir, 'train2014_urls.txt')  # this file includes urls for the training images\n",
        "    with open(train_url_file, 'r') as f:\n",
        "        train_urls = np.asarray([line.strip() for line in f])\n",
        "    data['train_urls'] = train_urls\n",
        "\n",
        "    val_url_file = os.path.join(data_dir, 'val2014_urls.txt')  # this file includes urls for the validation images\n",
        "    with open(val_url_file, 'r') as f:\n",
        "        val_urls = np.asarray([line.strip() for line in f])\n",
        "    data['val_urls'] = val_urls\n",
        "\n",
        "    # Maybe subsample the training data\n",
        "    if max_train is not None:\n",
        "        num_train = data['train_captions'].shape[0]\n",
        "        mask = np.random.randint(num_train, size=max_train)\n",
        "        data['train_captions'] = data['train_captions'][mask]\n",
        "        data['train_image_idxs'] = data['train_image_idxs'][mask]\n",
        "\n",
        "    return data\n",
        "\n",
        "data = load_coco_dataset(PCA_features=True)\n",
        "subset_data = load_coco_dataset(max_train=10000)\n",
        "data['train_captions'].shape, data['train_features'].shape\n",
        "\n",
        "\n",
        "sub_rnn_model = RNNImageCaption(cell_type='rnn', word_to_idx=data['word_to_idx'],\n",
        "          input_dim=data['train_features'].shape[1], hidden_dim=512, wordvec_dim=256,)\n",
        "# train model\n",
        "sub_rnn_solver = CaptionTrain(subset_data, sub_rnn_model, update='adam', num_epochs=50,\n",
        "                                batch_size=100, update_params={'lr': 5e-3}, lr_decay=0.95, print_freq=100)\n",
        "sub_rnn_solver.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd95ggcgEDWH",
        "outputId": "0ade094b-e9e3-4e5d-8123-ce1a21d62bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Iteration 1 / 41350) loss: 76.808959\n",
            "(Iteration 101 / 41350) loss: 69.643441\n",
            "(Iteration 201 / 41350) loss: 64.993081\n",
            "(Iteration 301 / 41350) loss: 58.692508\n",
            "(Iteration 401 / 41350) loss: 55.621276\n",
            "(Iteration 501 / 41350) loss: 52.129130\n",
            "(Iteration 601 / 41350) loss: 52.450638\n",
            "(Iteration 701 / 41350) loss: 51.360152\n",
            "(Iteration 801 / 41350) loss: 50.882756\n",
            "(Iteration 901 / 41350) loss: 48.966848\n",
            "(Iteration 1001 / 41350) loss: 48.850169\n",
            "(Iteration 1101 / 41350) loss: 48.630443\n",
            "(Iteration 1201 / 41350) loss: 47.669474\n",
            "(Iteration 1301 / 41350) loss: 48.499142\n",
            "(Iteration 1401 / 41350) loss: 48.282832\n",
            "(Iteration 1501 / 41350) loss: 47.180309\n",
            "(Iteration 1601 / 41350) loss: 45.866480\n",
            "(Iteration 1701 / 41350) loss: 47.075227\n",
            "(Iteration 1801 / 41350) loss: 46.000001\n",
            "(Iteration 1901 / 41350) loss: 45.668971\n",
            "(Iteration 2001 / 41350) loss: 44.584328\n",
            "(Iteration 2101 / 41350) loss: 47.253761\n",
            "(Iteration 2201 / 41350) loss: 45.015524\n",
            "(Iteration 2301 / 41350) loss: 44.012747\n",
            "(Iteration 2401 / 41350) loss: 44.980244\n",
            "(Iteration 2501 / 41350) loss: 44.415577\n",
            "(Iteration 2601 / 41350) loss: 43.155572\n",
            "(Iteration 2701 / 41350) loss: 42.494837\n",
            "(Iteration 2801 / 41350) loss: 43.172456\n",
            "(Iteration 2901 / 41350) loss: 45.271351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ablation"
      ],
      "metadata": {
        "id": "3GJkyTYYJZjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make directory and get annotations for training and testing\n",
        "!mkdir data\n",
        "!wget http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip -P ./data/\n",
        "!unzip ./data/captions_train-val2014.zip -d ./data/\n",
        "!rm ./data/captions_train-val2014.zip\n",
        "\n",
        "!mkdir data/images\n",
        "!mkdir data/images/train\n",
        "!mkdir data/images/test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC_5r9noBzfH",
        "outputId": "0dddc686-09f4-4a58-f8b0-6fcf568e6335"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "--2022-08-23 14:07:35--  http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip\n",
            "Resolving msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)... 20.60.195.163\n",
            "Connecting to msvocds.blob.core.windows.net (msvocds.blob.core.windows.net)|20.60.195.163|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19673183 (19M) [application/octet-stream Charset=UTF-8]\n",
            "Saving to: ‘./data/captions_train-val2014.zip’\n",
            "\n",
            "captions_train-val2 100%[===================>]  18.76M  5.05MB/s    in 4.8s    \n",
            "\n",
            "2022-08-23 14:07:40 (3.94 MB/s) - ‘./data/captions_train-val2014.zip’ saved [19673183/19673183]\n",
            "\n",
            "Archive:  ./data/captions_train-val2014.zip\n",
            "replace ./data/annotations/captions_train2014.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: ./data/annotations/captions_train2014.json  \n",
            "  inflating: ./data/annotations/captions_val2014.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data/annotations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubwC_k-uCOzp",
        "outputId": "cb0ccf9d-0b73-4c42-ee24-88c9c6c887c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "captions_train2014.json  captions_val2014.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from shutil import copyfile\n",
        "from pycocotools.coco import COCO\n",
        "from tqdm import tqdm\n",
        "\n",
        "coco = COCO('./data/annotations/captions_train2014.json')\n",
        "\n",
        "#get ids of training images\n",
        "with open('TrainImageIds.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    trainIds = list(reader)\n",
        "    \n",
        "trainIds = [int(i) for i in trainIds[0]]\n",
        "print(len(trainIds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "txzB7fC3BqQK",
        "outputId": "4a17ec57-12e5-4fd7-8b17-084d5845cf52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.89s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0d2acc588edf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#get ids of training images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TrainImageIds.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrainIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'TrainImageIds.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7rpkch0IBqUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hyR5tOclBqZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m_0T6rOe6fkO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size = 1024):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        \n",
        "        # get the pretrained densenet model\n",
        "        self.densenet = models.densenet121(pretrained=True)\n",
        "        \n",
        "        # replace the classifier with a fully connected embedding layer\n",
        "        self.densenet.classifier = nn.Linear(in_features=1024, out_features=1024)\n",
        "        \n",
        "        # add another fully connected layer\n",
        "        self.embed = nn.Linear(in_features=1024, out_features=embed_size)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        \n",
        "        # activation layers\n",
        "        self.prelu = nn.PReLU()\n",
        "        \n",
        "    def forward(self, images):\n",
        "        \n",
        "        # get the embeddings from the densenet\n",
        "        densenet_outputs = self.dropout(self.prelu(self.densenet(images)))\n",
        "        \n",
        "        # pass through the fully connected\n",
        "        embeddings = self.embed(densenet_outputs)\n",
        "        \n",
        "        return embeddings\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        # define the properties\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # lstm cell\n",
        "        self.lstm_cell = nn.LSTMCell(input_size=embed_size, hidden_size=hidden_size)\n",
        "    \n",
        "        # output fully connected layer\n",
        "        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
        "    \n",
        "        # embedding layer\n",
        "        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n",
        "    \n",
        "        # activations\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    \n",
        "    def forward(self, features, captions):\n",
        "        \n",
        "        # batch size\n",
        "        batch_size = features.size(0)\n",
        "        \n",
        "        # init the hidden and cell states to zeros\n",
        "        hidden_state = torch.zeros((batch_size, self.hidden_size)).cuda()\n",
        "        cell_state = torch.zeros((batch_size, self.hidden_size)).cuda()\n",
        "    \n",
        "        # define the output tensor placeholder\n",
        "        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).cuda()\n",
        "\n",
        "        # embed the captions\n",
        "        captions_embed = self.embed(captions)\n",
        "        \n",
        "        # pass the caption word by word\n",
        "        for t in range(captions.size(1)):\n",
        "\n",
        "            # for the first time step the input is the feature vector\n",
        "            if t == 0:\n",
        "                hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))\n",
        "                \n",
        "            # for the 2nd+ time step, using teacher forcer\n",
        "            else:\n",
        "                hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))\n",
        "            \n",
        "            # output of the attention mechanism\n",
        "            out = self.fc_out(hidden_state)\n",
        "            \n",
        "            # build the output tensor\n",
        "            outputs[:, t, :] = out\n",
        "    \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the losses for vizualization\n",
        "losses = list()\n",
        "val_losses = list()\n",
        "\n",
        "for epoch in range(1, 10+1):\n",
        "    \n",
        "    for i_step in range(1, total_step+1):\n",
        "        \n",
        "        # zero the gradients\n",
        "        decoder.zero_grad()\n",
        "        encoder.zero_grad()\n",
        "        \n",
        "        # set decoder and encoder into train mode\n",
        "        encoder.train()\n",
        "        decoder.train()\n",
        "        \n",
        "        # Randomly sample a caption length, and sample indices with that length.\n",
        "        indices = train_data_loader.dataset.get_train_indices()\n",
        "        \n",
        "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
        "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
        "        train_data_loader.batch_sampler.sampler = new_sampler\n",
        "        \n",
        "        # Obtain the batch.\n",
        "        images, captions = next(iter(train_data_loader))\n",
        "        \n",
        "        # make the captions for targets and teacher forcer\n",
        "        captions_target = captions[:, 1:].to(device)\n",
        "        captions_train = captions[:, :captions.shape[1]-1].to(device)\n",
        "\n",
        "        # Move batch of images and captions to GPU if CUDA is available.\n",
        "        images = images.to(device)\n",
        "        \n",
        "        # Pass the inputs through the CNN-RNN model.\n",
        "        features = encoder(images)\n",
        "        outputs = decoder(features, captions_train)\n",
        "        \n",
        "        # Calculate the batch loss\n",
        "        loss = criterion(outputs.view(-1, vocab_size), captions_target.contiguous().view(-1))\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update the parameters in the optimizer\n",
        "        optimizer.step()\n",
        "        \n",
        "        # - - - Validate - - -\n",
        "        # turn the evaluation mode on\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            # set the evaluation mode\n",
        "            encoder.eval()\n",
        "            decoder.eval()\n",
        "\n",
        "            # get the validation images and captions\n",
        "            val_images, val_captions = next(iter(val_data_loader))\n",
        "\n",
        "            # define the captions\n",
        "            captions_target = val_captions[:, 1:].to(device)\n",
        "            captions_train = val_captions[:, :val_captions.shape[1]-1].to(device)\n",
        "\n",
        "            # Move batch of images and captions to GPU if CUDA is available.\n",
        "            val_images = val_images.to(device)\n",
        "\n",
        "            # Pass the inputs through the CNN-RNN model.\n",
        "            features = encoder(val_images)\n",
        "            outputs = decoder(features, captions_train)\n",
        "\n",
        "            # Calculate the batch loss.\n",
        "            val_loss = criterion(outputs.view(-1, vocab_size), captions_target.contiguous().view(-1))\n",
        "        \n",
        "        # append the validation loss and training loss\n",
        "        val_losses.append(val_loss.item())\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        # save the losses\n",
        "        np.save('losses', np.array(losses))\n",
        "        np.save('val_losses', np.array(val_losses))\n",
        "        \n",
        "        # Get training statistics.\n",
        "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Val Loss: %.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), val_loss.item())\n",
        "        \n",
        "        # Print training statistics (on same line).\n",
        "        print('\\r' + stats, end=\"\")\n",
        "        sys.stdout.flush()\n",
        "            \n",
        "    # Save the weights.\n",
        "    if epoch % save_every == 0:\n",
        "        print(\"\\nSaving the model\")\n",
        "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pth' % epoch))\n",
        "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pth' % epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "nXIqA--j9swI",
        "outputId": "b9b157ee-b93f-44f3-a948-76682d3f281f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8fda72bb57a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_step\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'total_step' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_GRjFTr2_TM3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}